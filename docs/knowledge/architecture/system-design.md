# MIRIXç³»ç»Ÿæ¶æ„è®¾è®¡æ–‡æ¡£

## æ¶æ„æ¦‚è§ˆ

MIRIXæ˜¯ä¸€ä¸ªåŸºäºå¤šæ™ºèƒ½ä½“è®°å¿†ç³»ç»Ÿçš„ä¸ªäººåŠ©æ‰‹å¹³å°ï¼Œé‡‡ç”¨åˆ†å±‚å¾®æœåŠ¡æ¶æ„ï¼Œæ”¯æŒå¤šæ¨¡æ€è¾“å…¥ã€éšç§ä¼˜å…ˆè®¾è®¡å’Œå®æ—¶äº¤äº’ã€‚ç³»ç»Ÿé€šè¿‡å…­å±‚è®°å¿†æ¶æ„å’Œä¹ç§ä¸“ä¸šåŒ–æ™ºèƒ½ä½“ï¼Œæä¾›æ™ºèƒ½åŒ–çš„ä¸ªäººåŠ©æ‰‹æœåŠ¡ã€‚

**æ¶æ„ç‰ˆæœ¬**: v0.1.4  
**è®¾è®¡åŸåˆ™**: æ¨¡å—åŒ–ã€å¯æ‰©å±•ã€éšç§ä¼˜å…ˆã€é«˜æ€§èƒ½  
**éƒ¨ç½²æ¨¡å¼**: æœ¬åœ°éƒ¨ç½²ã€äº‘ç«¯éƒ¨ç½²ã€æ··åˆéƒ¨ç½²  

---

## ç¬¬ä¸€å±‚ï¼šæ•´ä½“æ¶æ„å¤§çº²

### ğŸ—ï¸ ç³»ç»Ÿæ¶æ„å…¨æ™¯å›¾

```mermaid
graph TB
    subgraph "ç”¨æˆ·ç•Œé¢å±‚ - User Interface Layer"
        A[Webå‰ç«¯ç•Œé¢] --> B[Electronæ¡Œé¢åº”ç”¨]
        B --> C[ç§»åŠ¨ç«¯åº”ç”¨]
        C --> D[å±å¹•æ´»åŠ¨ç›‘æ§]
    end
    
    subgraph "APIç½‘å…³å±‚ - API Gateway Layer"
        E[FastAPIä¸»æœåŠ¡] --> F[MCP SSEæœåŠ¡]
        F --> G[è®¤è¯æˆæƒä¸­é—´ä»¶]
        G --> H[é™æµå’Œç›‘æ§ä¸­é—´ä»¶]
    end
    
    subgraph "æ™ºèƒ½ä½“å±‚ - Agent Layer"
        I[Agentæ ¸å¿ƒå¼•æ“] --> J[9ç§ä¸“ä¸šåŒ–æ™ºèƒ½ä½“]
        J --> K[æ™ºèƒ½ä½“åä½œç®¡ç†å™¨]
        K --> L[LLMé›†æˆå±‚]
    end
    
    subgraph "è®°å¿†ç³»ç»Ÿå±‚ - Memory System Layer"
        M[è®°å¿†ç®¡ç†å™¨é›†ç¾¤] --> N[å…­å±‚è®°å¿†æ¶æ„]
        N --> O[æ··åˆæœç´¢å¼•æ“]
        O --> P[å‘é‡æ•°æ®åº“]
    end
    
    subgraph "æœåŠ¡å±‚ - Service Layer"
        Q[ä¸šåŠ¡æœåŠ¡ç®¡ç†å™¨] --> R[æ–‡æ¡£å¤„ç†æœåŠ¡]
        R --> S[MCPåè®®æœåŠ¡]
        S --> T[å·¥å…·é›†æˆæœåŠ¡]
    end
    
    subgraph "æ•°æ®è®¿é—®å±‚ - Data Access Layer"
        U[ORMæ•°æ®è®¿é—®å±‚] --> V[PostgreSQLä¸»æ•°æ®åº“]
        V --> W[Redisç¼“å­˜å±‚]
        W --> X[pgvectorå‘é‡å­˜å‚¨]
    end
    
    subgraph "åŸºç¡€è®¾æ–½å±‚ - Infrastructure Layer"
        Y[Dockerå®¹å™¨åŒ–] --> Z[æ—¥å¿—ç›‘æ§ç³»ç»Ÿ]
        Z --> AA[é…ç½®ç®¡ç†]
        AA --> BB[å®‰å…¨å’Œå¤‡ä»½]
    end
    
    A --> E
    E --> I
    I --> M
    M --> Q
    Q --> U
    U --> Y
```

### ğŸ¯ æ ¸å¿ƒè®¾è®¡åŸåˆ™

#### 1. æ¨¡å—åŒ–è®¾è®¡åŸåˆ™
- **é«˜å†…èšä½è€¦åˆ**ï¼šæ¯ä¸ªæ¨¡å—èŒè´£å•ä¸€ï¼Œæ¨¡å—é—´ä¾èµ–æœ€å°åŒ–
- **æ¥å£æ ‡å‡†åŒ–**ï¼šç»Ÿä¸€çš„æ¥å£è§„èŒƒå’Œæ•°æ®æ ¼å¼
- **å¯æ’æ‹”æ¶æ„**ï¼šæ”¯æŒç»„ä»¶çš„åŠ¨æ€åŠ è½½å’Œæ›¿æ¢
- **æœåŠ¡åŒ–æ‹†åˆ†**ï¼šæŒ‰ä¸šåŠ¡é¢†åŸŸæ‹†åˆ†ç‹¬ç«‹æœåŠ¡

#### 2. å¯æ‰©å±•æ€§åŸåˆ™
- **æ°´å¹³æ‰©å±•**ï¼šæ”¯æŒå¤šå®ä¾‹éƒ¨ç½²å’Œè´Ÿè½½å‡è¡¡
- **å‚ç›´æ‰©å±•**ï¼šæ”¯æŒå•å®ä¾‹æ€§èƒ½ä¼˜åŒ–å’Œèµ„æºæ‰©å®¹
- **åŠŸèƒ½æ‰©å±•**ï¼šæ”¯æŒæ–°æ™ºèƒ½ä½“ç±»å‹å’Œè®°å¿†æ¨¡å‹çš„æ·»åŠ 
- **åè®®æ‰©å±•**ï¼šæ”¯æŒæ–°çš„é€šä¿¡åè®®å’Œæ•°æ®æ ¼å¼

#### 3. éšç§ä¼˜å…ˆåŸåˆ™
- **æœ¬åœ°ä¼˜å…ˆ**ï¼šæ ¸å¿ƒæ•°æ®å’Œå¤„ç†ä¼˜å…ˆåœ¨æœ¬åœ°è¿›è¡Œ
- **æ•°æ®åŠ å¯†**ï¼šæ•æ„Ÿæ•°æ®çš„ä¼ è¾“å’Œå­˜å‚¨åŠ å¯†
- **è®¿é—®æ§åˆ¶**ï¼šç»†ç²’åº¦çš„æƒé™ç®¡ç†å’Œè®¿é—®æ§åˆ¶
- **å®¡è®¡æ—¥å¿—**ï¼šå®Œæ•´çš„æ“ä½œå®¡è®¡å’Œæ•°æ®è®¿é—®è®°å½•

#### 4. é«˜æ€§èƒ½åŸåˆ™
- **å¼‚æ­¥å¤„ç†**ï¼šå……åˆ†åˆ©ç”¨å¼‚æ­¥ç¼–ç¨‹æé«˜å¹¶å‘æ€§èƒ½
- **ç¼“å­˜ç­–ç•¥**ï¼šå¤šå±‚ç¼“å­˜å‡å°‘æ•°æ®åº“è®¿é—®
- **è¿æ¥æ± **ï¼šæ•°æ®åº“å’ŒHTTPè¿æ¥æ± ä¼˜åŒ–
- **æµå¼å¤„ç†**ï¼šå¤§æ•°æ®é‡çš„æµå¼ä¼ è¾“å’Œå¤„ç†

### ğŸ“Š æŠ€æœ¯æ ˆé€‰å‹

#### æ ¸å¿ƒæŠ€æœ¯æ ˆ
```yaml
ç¼–ç¨‹è¯­è¨€:
  - Python 3.11+: ä¸»è¦å¼€å‘è¯­è¨€
  - TypeScript: å‰ç«¯å¼€å‘è¯­è¨€
  - SQL: æ•°æ®åº“æŸ¥è¯¢è¯­è¨€

Webæ¡†æ¶:
  - FastAPI: é«˜æ€§èƒ½å¼‚æ­¥Webæ¡†æ¶
  - React: å‰ç«¯UIæ¡†æ¶
  - Electron: æ¡Œé¢åº”ç”¨æ¡†æ¶

æ•°æ®å­˜å‚¨:
  - PostgreSQL 16+: ä¸»æ•°æ®åº“
  - pgvector: å‘é‡æ•°æ®åº“æ‰©å±•
  - Redis 7+: ç¼“å­˜å’Œä¼šè¯å­˜å‚¨
  - SQLite: æœ¬åœ°å¼€å‘æ•°æ®åº“

AI/MLæŠ€æœ¯:
  - OpenAI GPT-4: ä¸»è¦LLMæ¨¡å‹
  - Anthropic Claude: å¤‡é€‰LLMæ¨¡å‹
  - Sentence Transformers: æ–‡æœ¬å‘é‡åŒ–
  - LangChain: LLMåº”ç”¨æ¡†æ¶

é€šä¿¡åè®®:
  - HTTP/HTTPS: RESTful API
  - WebSocket: å®æ—¶åŒå‘é€šä¿¡
  - Server-Sent Events: æµå¼æ•°æ®æ¨é€
  - MCP Protocol: æ¨¡å‹ä¸Šä¸‹æ–‡åè®®

éƒ¨ç½²è¿ç»´:
  - Docker: å®¹å™¨åŒ–éƒ¨ç½²
  - Docker Compose: æœ¬åœ°å¼€å‘ç¯å¢ƒ
  - Nginx: åå‘ä»£ç†å’Œè´Ÿè½½å‡è¡¡
  - Prometheus + Grafana: ç›‘æ§å‘Šè­¦
```

### ğŸ”„ æ•°æ®æµæ¶æ„

#### æ ¸å¿ƒæ•°æ®æµå›¾
```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant UI as ç”¨æˆ·ç•Œé¢
    participant API as APIç½‘å…³
    participant Agent as æ™ºèƒ½ä½“å¼•æ“
    participant Memory as è®°å¿†ç³»ç»Ÿ
    participant LLM as LLMæœåŠ¡
    participant DB as æ•°æ®åº“
    
    User->>UI: è¾“å…¥æ¶ˆæ¯/ä¸Šä¼ æ–‡æ¡£
    UI->>API: HTTP/WebSocketè¯·æ±‚
    API->>Agent: è·¯ç”±åˆ°å¯¹åº”æ™ºèƒ½ä½“
    
    Agent->>Memory: æŸ¥è¯¢ç›¸å…³è®°å¿†
    Memory->>DB: æ‰§è¡Œæ··åˆæœç´¢
    DB-->>Memory: è¿”å›æœç´¢ç»“æœ
    Memory-->>Agent: è¿”å›ç›¸å…³è®°å¿†
    
    Agent->>LLM: æ„å»ºæç¤ºè¯å¹¶è°ƒç”¨
    LLM-->>Agent: è¿”å›ç”Ÿæˆå†…å®¹
    
    Agent->>Memory: å­˜å‚¨æ–°è®°å¿†
    Memory->>DB: æŒä¹…åŒ–è®°å¿†æ•°æ®
    
    Agent-->>API: è¿”å›å“åº”ï¼ˆæµå¼ï¼‰
    API-->>UI: SSEæµå¼å“åº”
    UI-->>User: å®æ—¶æ˜¾ç¤ºç»“æœ
    
    Note over User,DB: å…¨æµç¨‹å¼‚æ­¥å¤„ç†ï¼Œæ”¯æŒå®æ—¶äº¤äº’
```

#### è®°å¿†ç³»ç»Ÿæ•°æ®æµ
```mermaid
graph LR
    subgraph "è¾“å…¥å¤„ç†"
        A[ç”¨æˆ·è¾“å…¥] --> B[å†…å®¹è§£æ]
        B --> C[è¯­ä¹‰åˆ†æ]
        C --> D[å‘é‡åŒ–å¤„ç†]
    end
    
    subgraph "è®°å¿†æ£€ç´¢"
        E[æŸ¥è¯¢æ„å»º] --> F[æ··åˆæœç´¢]
        F --> G[BM25æ–‡æœ¬æœç´¢]
        F --> H[å‘é‡ç›¸ä¼¼åº¦æœç´¢]
        G --> I[ç»“æœèåˆæ’åº]
        H --> I
    end
    
    subgraph "è®°å¿†å­˜å‚¨"
        J[è®°å¿†åˆ†ç±»] --> K[Core Memory]
        J --> L[Episodic Memory]
        J --> M[Semantic Memory]
        J --> N[Procedural Memory]
        J --> O[Resource Memory]
        J --> P[Knowledge Vault]
    end
    
    subgraph "æ™ºèƒ½ä½“å¤„ç†"
        Q[è®°å¿†æ•´åˆ] --> R[ä¸Šä¸‹æ–‡æ„å»º]
        R --> S[LLMæ¨ç†]
        S --> T[å“åº”ç”Ÿæˆ]
    end
    
    D --> E
    I --> Q
    T --> J
```

---

## ç¬¬äºŒå±‚ï¼šæŠ€æœ¯è®¾è®¡æ”¯æŒ

### ğŸ›ï¸ åˆ†å±‚æ¶æ„è®¾è®¡

#### 1. ç”¨æˆ·ç•Œé¢å±‚è®¾è®¡
```typescript
// å‰ç«¯æ¶æ„è®¾è®¡
interface UIArchitecture {
  // Reactç»„ä»¶æ¶æ„
  components: {
    layout: 'MainLayout' | 'ChatLayout' | 'SettingsLayout';
    pages: 'ChatPage' | 'AgentsPage' | 'MemoryPage' | 'SettingsPage';
    widgets: 'MessageWidget' | 'AgentWidget' | 'MemoryWidget';
  };
  
  // çŠ¶æ€ç®¡ç†
  stateManagement: {
    global: 'Redux Toolkit';
    local: 'React Hooks';
    persistence: 'Redux Persist';
  };
  
  // é€šä¿¡å±‚
  communication: {
    http: 'Axios';
    websocket: 'Socket.IO Client';
    sse: 'EventSource API';
  };
  
  // Electroné›†æˆ
  desktop: {
    main: 'Electron Main Process';
    renderer: 'React Renderer Process';
    ipc: 'Inter-Process Communication';
    native: 'Node.js Native Modules';
  };
}

// å±å¹•ç›‘æ§æ¶æ„
interface ScreenMonitoring {
  capture: {
    method: 'screenshot-desktop' | 'node-screenshots';
    frequency: number; // æˆªå›¾é¢‘ç‡ï¼ˆç§’ï¼‰
    quality: 'low' | 'medium' | 'high';
  };
  
  processing: {
    ocr: 'Tesseract.js';
    imageAnalysis: 'Canvas API';
    dataExtraction: 'Custom Algorithms';
  };
  
  privacy: {
    localProcessing: boolean;
    dataEncryption: boolean;
    userConsent: boolean;
  };
}
```

#### 2. APIç½‘å…³å±‚è®¾è®¡
```python
# APIç½‘å…³æ¶æ„è®¾è®¡
from typing import Protocol, Dict, Any
from abc import ABC, abstractmethod

class APIGatewayArchitecture:
    """APIç½‘å…³æ¶æ„è®¾è®¡"""
    
    def __init__(self):
        self.middleware_stack = [
            'CORSMiddleware',
            'LoggingMiddleware', 
            'AuthenticationMiddleware',
            'RateLimitingMiddleware',
            'ValidationMiddleware'
        ]
        
        self.routing_strategy = {
            'agents': 'AgentServiceRouter',
            'memory': 'MemoryServiceRouter',
            'documents': 'DocumentServiceRouter',
            'mcp': 'MCPServiceRouter'
        }
        
        self.response_formats = {
            'sync': 'JSONResponse',
            'async': 'StreamingResponse',
            'sse': 'ServerSentEventsResponse'
        }

class MiddlewareProtocol(Protocol):
    """ä¸­é—´ä»¶åè®®å®šä¹‰"""
    
    async def process_request(self, request: Any) -> Any:
        """å¤„ç†è¯·æ±‚"""
        ...
    
    async def process_response(self, response: Any) -> Any:
        """å¤„ç†å“åº”"""
        ...

class AuthenticationMiddleware:
    """è®¤è¯ä¸­é—´ä»¶å®ç°"""
    
    def __init__(self, secret_key: str):
        self.secret_key = secret_key
        self.excluded_paths = ['/health', '/docs', '/openapi.json']
    
    async def process_request(self, request):
        """å¤„ç†è®¤è¯é€»è¾‘"""
        if request.url.path in self.excluded_paths:
            return request
        
        # APIå¯†é’¥éªŒè¯
        api_key = request.headers.get('X-API-Key')
        if not api_key or not self.validate_api_key(api_key):
            raise HTTPException(status_code=401, detail="æ— æ•ˆçš„APIå¯†é’¥")
        
        return request
    
    def validate_api_key(self, api_key: str) -> bool:
        """éªŒè¯APIå¯†é’¥"""
        # å®ç°APIå¯†é’¥éªŒè¯é€»è¾‘
        return True

class RateLimitingMiddleware:
    """é™æµä¸­é—´ä»¶å®ç°"""
    
    def __init__(self, redis_client, requests_per_minute: int = 60):
        self.redis = redis_client
        self.requests_per_minute = requests_per_minute
    
    async def process_request(self, request):
        """å¤„ç†é™æµé€»è¾‘"""
        client_ip = request.client.host
        key = f"rate_limit:{client_ip}"
        
        current_requests = await self.redis.get(key)
        if current_requests and int(current_requests) >= self.requests_per_minute:
            raise HTTPException(status_code=429, detail="è¯·æ±‚é¢‘ç‡è¶…é™")
        
        # å¢åŠ è¯·æ±‚è®¡æ•°
        await self.redis.incr(key)
        await self.redis.expire(key, 60)  # 1åˆ†é’Ÿè¿‡æœŸ
        
        return request
```

#### 3. æ™ºèƒ½ä½“å±‚æ¶æ„è®¾è®¡
```python
# æ™ºèƒ½ä½“å±‚æ¶æ„è®¾è®¡
from enum import Enum
from typing import Dict, List, Optional, Any
from abc import ABC, abstractmethod

class AgentType(Enum):
    """æ™ºèƒ½ä½“ç±»å‹æšä¸¾"""
    CORE_MEMORY = "core_memory"
    EPISODIC_MEMORY = "episodic_memory"
    SEMANTIC_MEMORY = "semantic_memory"
    PROCEDURAL_MEMORY = "procedural_memory"
    RESOURCE_MEMORY = "resource_memory"
    KNOWLEDGE_VAULT = "knowledge_vault"
    CHAT = "chat"
    SEARCH = "search"
    TOOL = "tool"

class AgentArchitecture:
    """æ™ºèƒ½ä½“æ¶æ„è®¾è®¡"""
    
    def __init__(self):
        self.agent_registry = {}
        self.collaboration_patterns = {
            'sequential': 'SequentialCollaboration',
            'parallel': 'ParallelCollaboration',
            'hierarchical': 'HierarchicalCollaboration',
            'peer_to_peer': 'PeerToPeerCollaboration'
        }
        
        self.communication_protocols = {
            'message_passing': 'MessagePassingProtocol',
            'shared_memory': 'SharedMemoryProtocol',
            'event_driven': 'EventDrivenProtocol'
        }

class BaseAgent(ABC):
    """æ™ºèƒ½ä½“åŸºç±»"""
    
    def __init__(self, agent_id: str, agent_type: AgentType):
        self.agent_id = agent_id
        self.agent_type = agent_type
        self.state = AgentState()
        self.memory_managers = {}
        self.tools = []
    
    @abstractmethod
    async def process_message(self, message: str, context: Dict[str, Any]) -> str:
        """å¤„ç†æ¶ˆæ¯çš„æŠ½è±¡æ–¹æ³•"""
        pass
    
    @abstractmethod
    async def collaborate(self, other_agents: List['BaseAgent'], task: str) -> Any:
        """æ™ºèƒ½ä½“åä½œçš„æŠ½è±¡æ–¹æ³•"""
        pass
    
    async def update_memory(self, memory_type: str, content: Any):
        """æ›´æ–°è®°å¿†"""
        if memory_type in self.memory_managers:
            await self.memory_managers[memory_type].store(content)
    
    async def retrieve_memory(self, query: str, memory_type: str = None) -> List[Any]:
        """æ£€ç´¢è®°å¿†"""
        if memory_type and memory_type in self.memory_managers:
            return await self.memory_managers[memory_type].search(query)
        
        # è·¨è®°å¿†ç±»å‹æœç´¢
        results = []
        for manager in self.memory_managers.values():
            results.extend(await manager.search(query))
        
        return results

class AgentCollaborationManager:
    """æ™ºèƒ½ä½“åä½œç®¡ç†å™¨"""
    
    def __init__(self):
        self.active_collaborations = {}
        self.collaboration_history = []
    
    async def orchestrate_collaboration(
        self, 
        agents: List[BaseAgent], 
        task: str, 
        pattern: str = 'sequential'
    ) -> Any:
        """ç¼–æ’æ™ºèƒ½ä½“åä½œ"""
        
        collaboration_id = f"collab_{len(self.active_collaborations)}"
        
        if pattern == 'sequential':
            return await self._sequential_collaboration(agents, task, collaboration_id)
        elif pattern == 'parallel':
            return await self._parallel_collaboration(agents, task, collaboration_id)
        elif pattern == 'hierarchical':
            return await self._hierarchical_collaboration(agents, task, collaboration_id)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åä½œæ¨¡å¼: {pattern}")
    
    async def _sequential_collaboration(
        self, 
        agents: List[BaseAgent], 
        task: str, 
        collaboration_id: str
    ) -> Any:
        """é¡ºåºåä½œæ¨¡å¼"""
        result = task
        context = {"collaboration_id": collaboration_id, "step": 0}
        
        for agent in agents:
            context["step"] += 1
            result = await agent.process_message(result, context)
            
            # è®°å½•åä½œæ­¥éª¤
            self.collaboration_history.append({
                "collaboration_id": collaboration_id,
                "agent_id": agent.agent_id,
                "step": context["step"],
                "input": task if context["step"] == 1 else "previous_result",
                "output": result
            })
        
        return result
    
    async def _parallel_collaboration(
        self, 
        agents: List[BaseAgent], 
        task: str, 
        collaboration_id: str
    ) -> Any:
        """å¹¶è¡Œåä½œæ¨¡å¼"""
        import asyncio
        
        context = {"collaboration_id": collaboration_id, "mode": "parallel"}
        
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æ™ºèƒ½ä½“
        tasks = [agent.process_message(task, context) for agent in agents]
        results = await asyncio.gather(*tasks)
        
        # åˆå¹¶ç»“æœ
        merged_result = self._merge_parallel_results(results, agents)
        
        return merged_result
    
    def _merge_parallel_results(self, results: List[Any], agents: List[BaseAgent]) -> Any:
        """åˆå¹¶å¹¶è¡Œåä½œç»“æœ"""
        # æ ¹æ®æ™ºèƒ½ä½“ç±»å‹å’Œç»“æœç±»å‹è¿›è¡Œæ™ºèƒ½åˆå¹¶
        merged = {
            "collaboration_type": "parallel",
            "agent_results": {}
        }
        
        for agent, result in zip(agents, results):
            merged["agent_results"][agent.agent_id] = {
                "agent_type": agent.agent_type.value,
                "result": result
            }
        
        return merged
```

#### 4. è®°å¿†ç³»ç»Ÿæ¶æ„è®¾è®¡
```python
# è®°å¿†ç³»ç»Ÿæ¶æ„è®¾è®¡
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass
from enum import Enum

class MemoryType(Enum):
    """è®°å¿†ç±»å‹æšä¸¾"""
    CORE = "core"
    EPISODIC = "episodic"
    SEMANTIC = "semantic"
    PROCEDURAL = "procedural"
    RESOURCE = "resource"
    KNOWLEDGE_VAULT = "knowledge_vault"

@dataclass
class MemoryItem:
    """è®°å¿†é¡¹æ•°æ®ç»“æ„"""
    id: str
    content: str
    memory_type: MemoryType
    embedding: Optional[List[float]]
    metadata: Dict[str, Any]
    created_at: str
    updated_at: str
    relevance_score: Optional[float] = None

class MemorySystemArchitecture:
    """è®°å¿†ç³»ç»Ÿæ¶æ„è®¾è®¡"""
    
    def __init__(self):
        self.memory_managers = {}
        self.search_engine = HybridSearchEngine()
        self.embedding_service = EmbeddingService()
        self.memory_consolidation = MemoryConsolidationService()
    
    def register_memory_manager(self, memory_type: MemoryType, manager):
        """æ³¨å†Œè®°å¿†ç®¡ç†å™¨"""
        self.memory_managers[memory_type] = manager
    
    async def store_memory(
        self, 
        content: str, 
        memory_type: MemoryType, 
        metadata: Dict[str, Any] = None
    ) -> MemoryItem:
        """å­˜å‚¨è®°å¿†"""
        
        # ç”Ÿæˆå‘é‡åµŒå…¥
        embedding = await self.embedding_service.generate_embedding(content)
        
        # åˆ›å»ºè®°å¿†é¡¹
        memory_item = MemoryItem(
            id=f"{memory_type.value}_{uuid.uuid4()}",
            content=content,
            memory_type=memory_type,
            embedding=embedding,
            metadata=metadata or {},
            created_at=datetime.utcnow().isoformat(),
            updated_at=datetime.utcnow().isoformat()
        )
        
        # å­˜å‚¨åˆ°å¯¹åº”çš„è®°å¿†ç®¡ç†å™¨
        if memory_type in self.memory_managers:
            await self.memory_managers[memory_type].store(memory_item)
        
        return memory_item
    
    async def search_memory(
        self, 
        query: str, 
        memory_types: List[MemoryType] = None,
        limit: int = 10
    ) -> List[MemoryItem]:
        """æœç´¢è®°å¿†"""
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = await self.embedding_service.generate_embedding(query)
        
        # æ‰§è¡Œæ··åˆæœç´¢
        results = await self.search_engine.hybrid_search(
            query=query,
            query_embedding=query_embedding,
            memory_types=memory_types or list(MemoryType),
            limit=limit
        )
        
        return results

class HybridSearchEngine:
    """æ··åˆæœç´¢å¼•æ“"""
    
    def __init__(self):
        self.bm25_weight = 0.3
        self.vector_weight = 0.7
        self.rerank_model = None
    
    async def hybrid_search(
        self,
        query: str,
        query_embedding: List[float],
        memory_types: List[MemoryType],
        limit: int = 10
    ) -> List[MemoryItem]:
        """æ‰§è¡Œæ··åˆæœç´¢"""
        
        # BM25æ–‡æœ¬æœç´¢
        bm25_results = await self._bm25_search(query, memory_types, limit * 2)
        
        # å‘é‡ç›¸ä¼¼åº¦æœç´¢
        vector_results = await self._vector_search(query_embedding, memory_types, limit * 2)
        
        # ç»“æœèåˆå’Œé‡æ’åº
        merged_results = self._merge_and_rerank(bm25_results, vector_results, limit)
        
        return merged_results
    
    async def _bm25_search(
        self, 
        query: str, 
        memory_types: List[MemoryType], 
        limit: int
    ) -> List[MemoryItem]:
        """BM25æ–‡æœ¬æœç´¢"""
        # å®ç°BM25æœç´¢é€»è¾‘
        # è¿™é‡Œåº”è¯¥è°ƒç”¨PostgreSQLçš„å…¨æ–‡æœç´¢åŠŸèƒ½
        pass
    
    async def _vector_search(
        self, 
        query_embedding: List[float], 
        memory_types: List[MemoryType], 
        limit: int
    ) -> List[MemoryItem]:
        """å‘é‡ç›¸ä¼¼åº¦æœç´¢"""
        # å®ç°å‘é‡æœç´¢é€»è¾‘
        # è¿™é‡Œåº”è¯¥è°ƒç”¨pgvectorçš„ç›¸ä¼¼åº¦æœç´¢åŠŸèƒ½
        pass
    
    def _merge_and_rerank(
        self, 
        bm25_results: List[MemoryItem], 
        vector_results: List[MemoryItem], 
        limit: int
    ) -> List[MemoryItem]:
        """åˆå¹¶å’Œé‡æ’åºæœç´¢ç»“æœ"""
        
        # åˆ›å»ºç»“æœå­—å…¸ï¼Œé¿å…é‡å¤
        result_dict = {}
        
        # æ·»åŠ BM25ç»“æœ
        for i, item in enumerate(bm25_results):
            bm25_score = (len(bm25_results) - i) / len(bm25_results)
            result_dict[item.id] = {
                'item': item,
                'bm25_score': bm25_score,
                'vector_score': 0.0
            }
        
        # æ·»åŠ å‘é‡æœç´¢ç»“æœ
        for i, item in enumerate(vector_results):
            vector_score = (len(vector_results) - i) / len(vector_results)
            if item.id in result_dict:
                result_dict[item.id]['vector_score'] = vector_score
            else:
                result_dict[item.id] = {
                    'item': item,
                    'bm25_score': 0.0,
                    'vector_score': vector_score
                }
        
        # è®¡ç®—ç»¼åˆå¾—åˆ†å¹¶æ’åº
        for result in result_dict.values():
            result['final_score'] = (
                result['bm25_score'] * self.bm25_weight + 
                result['vector_score'] * self.vector_weight
            )
            result['item'].relevance_score = result['final_score']
        
        # æŒ‰ç»¼åˆå¾—åˆ†æ’åº
        sorted_results = sorted(
            result_dict.values(), 
            key=lambda x: x['final_score'], 
            reverse=True
        )
        
        return [result['item'] for result in sorted_results[:limit]]
```

### ğŸ”— é›†æˆæ–¹æ¡ˆè®¾è®¡

#### 1. LLMé›†æˆæ¶æ„
```python
# LLMé›†æˆæ¶æ„è®¾è®¡
from typing import Protocol, Dict, Any, List, Optional
from abc import ABC, abstractmethod

class LLMProvider(Protocol):
    """LLMæä¾›å•†åè®®"""
    
    async def generate(
        self, 
        messages: List[Dict[str, str]], 
        **kwargs
    ) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        ...
    
    async def generate_stream(
        self, 
        messages: List[Dict[str, str]], 
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """æµå¼ç”Ÿæˆæ–‡æœ¬"""
        ...
    
    async def generate_embedding(self, text: str) -> List[float]:
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥"""
        ...

class LLMIntegrationArchitecture:
    """LLMé›†æˆæ¶æ„"""
    
    def __init__(self):
        self.providers = {}
        self.default_provider = None
        self.fallback_providers = []
        self.load_balancer = LLMLoadBalancer()
    
    def register_provider(self, name: str, provider: LLMProvider, is_default: bool = False):
        """æ³¨å†ŒLLMæä¾›å•†"""
        self.providers[name] = provider
        if is_default:
            self.default_provider = name
    
    async def generate(
        self, 
        messages: List[Dict[str, str]], 
        provider: str = None,
        **kwargs
    ) -> str:
        """ç”Ÿæˆæ–‡æœ¬ï¼ˆå¸¦æ•…éšœè½¬ç§»ï¼‰"""
        
        target_provider = provider or self.default_provider
        
        try:
            return await self.providers[target_provider].generate(messages, **kwargs)
        except Exception as e:
            # æ•…éšœè½¬ç§»åˆ°å¤‡ç”¨æä¾›å•†
            for fallback in self.fallback_providers:
                try:
                    return await self.providers[fallback].generate(messages, **kwargs)
                except Exception:
                    continue
            
            raise e

class OpenAIProvider:
    """OpenAIæä¾›å•†å®ç°"""
    
    def __init__(self, api_key: str, model: str = "gpt-4"):
        self.api_key = api_key
        self.model = model
        self.client = openai.AsyncOpenAI(api_key=api_key)
    
    async def generate(self, messages: List[Dict[str, str]], **kwargs) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            **kwargs
        )
        return response.choices[0].message.content
    
    async def generate_stream(self, messages: List[Dict[str, str]], **kwargs):
        """æµå¼ç”Ÿæˆæ–‡æœ¬"""
        stream = await self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            stream=True,
            **kwargs
        )
        
        async for chunk in stream:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
    
    async def generate_embedding(self, text: str) -> List[float]:
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥"""
        response = await self.client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return response.data[0].embedding

class AnthropicProvider:
    """Anthropicæä¾›å•†å®ç°"""
    
    def __init__(self, api_key: str, model: str = "claude-3-sonnet-20240229"):
        self.api_key = api_key
        self.model = model
        self.client = anthropic.AsyncAnthropic(api_key=api_key)
    
    async def generate(self, messages: List[Dict[str, str]], **kwargs) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        anthropic_messages = self._convert_messages(messages)
        
        response = await self.client.messages.create(
            model=self.model,
            messages=anthropic_messages,
            **kwargs
        )
        return response.content[0].text
    
    def _convert_messages(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """è½¬æ¢æ¶ˆæ¯æ ¼å¼ä»¥é€‚é…Anthropic API"""
        converted = []
        for msg in messages:
            if msg["role"] == "system":
                # Anthropicå°†systemæ¶ˆæ¯åˆå¹¶åˆ°ç¬¬ä¸€ä¸ªuseræ¶ˆæ¯ä¸­
                continue
            converted.append(msg)
        return converted
```

#### 2. MCPåè®®é›†æˆ
```python
# MCPåè®®é›†æˆæ¶æ„
from typing import Dict, Any, List, Optional, Callable
import json
import asyncio

class MCPProtocolArchitecture:
    """MCPåè®®é›†æˆæ¶æ„"""
    
    def __init__(self):
        self.servers = {}
        self.tools = {}
        self.resources = {}
        self.prompts = {}
        self.event_handlers = {}
    
    async def register_server(self, server_name: str, server_config: Dict[str, Any]):
        """æ³¨å†ŒMCPæœåŠ¡å™¨"""
        server = MCPServer(server_name, server_config)
        await server.initialize()
        
        self.servers[server_name] = server
        
        # æ³¨å†ŒæœåŠ¡å™¨çš„å·¥å…·ã€èµ„æºå’Œæç¤º
        await self._register_server_capabilities(server_name, server)
    
    async def _register_server_capabilities(self, server_name: str, server: 'MCPServer'):
        """æ³¨å†ŒæœåŠ¡å™¨èƒ½åŠ›"""
        
        # æ³¨å†Œå·¥å…·
        tools = await server.list_tools()
        for tool in tools:
            tool_key = f"{server_name}.{tool['name']}"
            self.tools[tool_key] = {
                'server': server_name,
                'tool': tool,
                'handler': server.call_tool
            }
        
        # æ³¨å†Œèµ„æº
        resources = await server.list_resources()
        for resource in resources:
            resource_key = f"{server_name}.{resource['uri']}"
            self.resources[resource_key] = {
                'server': server_name,
                'resource': resource,
                'handler': server.read_resource
            }
        
        # æ³¨å†Œæç¤º
        prompts = await server.list_prompts()
        for prompt in prompts:
            prompt_key = f"{server_name}.{prompt['name']}"
            self.prompts[prompt_key] = {
                'server': server_name,
                'prompt': prompt,
                'handler': server.get_prompt
            }
    
    async def call_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Any:
        """è°ƒç”¨MCPå·¥å…·"""
        if tool_name not in self.tools:
            raise ValueError(f"å·¥å…·ä¸å­˜åœ¨: {tool_name}")
        
        tool_info = self.tools[tool_name]
        server = self.servers[tool_info['server']]
        
        return await server.call_tool(tool_info['tool']['name'], arguments)
    
    async def read_resource(self, resource_uri: str) -> Any:
        """è¯»å–MCPèµ„æº"""
        if resource_uri not in self.resources:
            raise ValueError(f"èµ„æºä¸å­˜åœ¨: {resource_uri}")
        
        resource_info = self.resources[resource_uri]
        server = self.servers[resource_info['server']]
        
        return await server.read_resource(resource_info['resource']['uri'])
    
    def find_tools_by_capability(self, capability: str) -> List[str]:
        """æ ¹æ®èƒ½åŠ›æŸ¥æ‰¾å·¥å…·"""
        matching_tools = []
        
        for tool_name, tool_info in self.tools.items():
            tool_description = tool_info['tool'].get('description', '').lower()
            if capability.lower() in tool_description:
                matching_tools.append(tool_name)
        
        return matching_tools

class MCPServer:
    """MCPæœåŠ¡å™¨å®¢æˆ·ç«¯"""
    
    def __init__(self, name: str, config: Dict[str, Any]):
        self.name = name
        self.config = config
        self.capabilities = {}
        self.session_id = None
    
    async def initialize(self):
        """åˆå§‹åŒ–MCPæœåŠ¡å™¨è¿æ¥"""
        # å‘é€åˆå§‹åŒ–è¯·æ±‚
        init_request = {
            "jsonrpc": "2.0",
            "id": 1,
            "method": "initialize",
            "params": {
                "protocolVersion": "2024-11-05",
                "capabilities": {
                    "tools": {"listChanged": True},
                    "resources": {"subscribe": True, "listChanged": True},
                    "prompts": {"listChanged": True}
                },
                "clientInfo": {
                    "name": "MIRIX",
                    "version": "0.1.4"
                }
            }
        }
        
        response = await self._send_request(init_request)
        self.capabilities = response.get("result", {}).get("capabilities", {})
    
    async def list_tools(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºå¯ç”¨å·¥å…·"""
        request = {
            "jsonrpc": "2.0",
            "id": 2,
            "method": "tools/list"
        }
        
        response = await self._send_request(request)
        return response.get("result", {}).get("tools", [])
    
    async def call_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Any:
        """è°ƒç”¨å·¥å…·"""
        request = {
            "jsonrpc": "2.0",
            "id": 3,
            "method": "tools/call",
            "params": {
                "name": tool_name,
                "arguments": arguments
            }
        }
        
        response = await self._send_request(request)
        return response.get("result")
    
    async def list_resources(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºå¯ç”¨èµ„æº"""
        request = {
            "jsonrpc": "2.0",
            "id": 4,
            "method": "resources/list"
        }
        
        response = await self._send_request(request)
        return response.get("result", {}).get("resources", [])
    
    async def read_resource(self, resource_uri: str) -> Any:
        """è¯»å–èµ„æº"""
        request = {
            "jsonrpc": "2.0",
            "id": 5,
            "method": "resources/read",
            "params": {
                "uri": resource_uri
            }
        }
        
        response = await self._send_request(request)
        return response.get("result")
    
    async def _send_request(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """å‘é€MCPè¯·æ±‚"""
        # è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„ç½‘ç»œé€šä¿¡
        # å¯ä»¥æ˜¯HTTPã€WebSocketæˆ–å…¶ä»–åè®®
        
        # æ¨¡æ‹Ÿå“åº”
        return {
            "jsonrpc": "2.0",
            "id": request["id"],
            "result": {}
        }
```

---

## ç¬¬ä¸‰å±‚ï¼šå®æ–½æŒ‡å¯¼

### ğŸš€ éƒ¨ç½²æ¶æ„å®æ–½

#### 1. Dockerå®¹å™¨åŒ–éƒ¨ç½²
```yaml
# docker-compose.yml - å®Œæ•´éƒ¨ç½²é…ç½®
version: '3.8'

services:
  # PostgreSQLæ•°æ®åº“
  postgres:
    image: pgvector/pgvector:pg16
    container_name: mirix-postgres
    environment:
      POSTGRES_DB: mirix
      POSTGRES_USER: mirix_user
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_INITDB_ARGS: "--encoding=UTF-8 --lc-collate=C --lc-ctype=C"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-scripts:/docker-entrypoint-initdb.d
    ports:
      - "5432:5432"
    networks:
      - mirix-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U mirix_user -d mirix"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Redisç¼“å­˜
  redis:
    image: redis:7-alpine
    container_name: mirix-redis
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    networks:
      - mirix-network
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # MIRIXåç«¯æœåŠ¡
  mirix-backend:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    container_name: mirix-backend
    environment:
      # æ•°æ®åº“é…ç½®
      DATABASE_URL: postgresql://mirix_user:${POSTGRES_PASSWORD}@postgres:5432/mirix
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379/0
      
      # LLMé…ç½®
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
      
      # æœåŠ¡é…ç½®
      HOST: 0.0.0.0
      PORT: 47283
      DEBUG: false
      LOG_LEVEL: INFO
      
      # å®‰å…¨é…ç½®
      SECRET_KEY: ${SECRET_KEY}
      API_KEY: ${API_KEY}
      
      # MCPé…ç½®
      MCP_SSE_URL: http://mcp-sse-service:8080
    volumes:
      - ./logs:/app/logs
      - ./uploads:/app/uploads
      - ./config:/app/config
    ports:
      - "47283:47283"
    networks:
      - mirix-network
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:47283/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # MCP SSEæœåŠ¡
  mcp-sse-service:
    build:
      context: ./mcp_sse_service
      dockerfile: Dockerfile
    container_name: mirix-mcp-sse
    environment:
      HOST: 0.0.0.0
      PORT: 8080
      DEBUG: false
      LOG_LEVEL: INFO
      MIRIX_BACKEND_URL: http://mirix-backend:47283
      ALLOWED_ORIGINS: "*"
      MCP_API_KEY: ${MCP_API_KEY}
    ports:
      - "8080:8080"
    networks:
      - mirix-network
    depends_on:
      - mirix-backend
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Nginxåå‘ä»£ç†
  nginx:
    image: nginx:alpine
    container_name: mirix-nginx
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/ssl:/etc/nginx/ssl
      - ./logs/nginx:/var/log/nginx
    ports:
      - "80:80"
      - "443:443"
    networks:
      - mirix-network
    depends_on:
      - mirix-backend
      - mcp-sse-service
    restart: unless-stopped

  # ç›‘æ§æœåŠ¡
  prometheus:
    image: prom/prometheus:latest
    container_name: mirix-prometheus
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    networks:
      - mirix-network
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    container_name: mirix-grafana
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources
    ports:
      - "3000:3000"
    networks:
      - mirix-network
    depends_on:
      - prometheus
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
  prometheus_data:
  grafana_data:

networks:
  mirix-network:
    driver: bridge
```

#### 2. Dockerfileä¼˜åŒ–é…ç½®
```dockerfile
# Dockerfile - å¤šé˜¶æ®µæ„å»º
FROM python:3.11-slim as base

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    libpq-dev \
    curl \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .
COPY requirements-dev.txt .

# å®‰è£…Pythonä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# å¼€å‘é˜¶æ®µ
FROM base as development

RUN pip install --no-cache-dir -r requirements-dev.txt

COPY . .

EXPOSE 47283

CMD ["python", "-m", "uvicorn", "mirix.server.fastapi_server:app", "--host", "0.0.0.0", "--port", "47283", "--reload"]

# ç”Ÿäº§é˜¶æ®µ
FROM base as production

# åˆ›å»ºérootç”¨æˆ·
RUN useradd --create-home --shell /bin/bash mirix

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY --chown=mirix:mirix . .

# åˆ›å»ºå¿…è¦ç›®å½•
RUN mkdir -p /app/logs /app/uploads /app/config && \
    chown -R mirix:mirix /app

# åˆ‡æ¢åˆ°érootç”¨æˆ·
USER mirix

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:47283/health || exit 1

EXPOSE 47283

CMD ["python", "-m", "uvicorn", "mirix.server.fastapi_server:app", "--host", "0.0.0.0", "--port", "47283", "--workers", "4"]
```

#### 3. ç¯å¢ƒé…ç½®ç®¡ç†
```bash
# .env.example - ç¯å¢ƒå˜é‡æ¨¡æ¿
# æ•°æ®åº“é…ç½®
POSTGRES_PASSWORD=your_postgres_password
REDIS_PASSWORD=your_redis_password
DATABASE_URL=postgresql://mirix_user:your_postgres_password@localhost:5432/mirix

# LLM APIå¯†é’¥
OPENAI_API_KEY=your_openai_api_key
ANTHROPIC_API_KEY=your_anthropic_api_key

# æœåŠ¡é…ç½®
SECRET_KEY=your_secret_key_here
API_KEY=your_api_key_here
MCP_API_KEY=your_mcp_api_key

# ç›‘æ§é…ç½®
GRAFANA_PASSWORD=your_grafana_password

# æ—¥å¿—é…ç½®
LOG_LEVEL=INFO
LOG_FORMAT=json

# æ€§èƒ½é…ç½®
WORKERS=4
MAX_CONNECTIONS=100
TIMEOUT=30
```

```python
# config/settings.py - é…ç½®ç®¡ç†
from typing import Optional, List
from pydantic_settings import BaseSettings
from pydantic import Field, validator

class Settings(BaseSettings):
    """åº”ç”¨é…ç½®"""
    
    # æœåŠ¡é…ç½®
    host: str = Field(default="0.0.0.0", env="HOST")
    port: int = Field(default=47283, env="PORT")
    debug: bool = Field(default=False, env="DEBUG")
    workers: int = Field(default=4, env="WORKERS")
    
    # æ•°æ®åº“é…ç½®
    database_url: str = Field(..., env="DATABASE_URL")
    redis_url: str = Field(..., env="REDIS_URL")
    
    # LLMé…ç½®
    openai_api_key: Optional[str] = Field(None, env="OPENAI_API_KEY")
    anthropic_api_key: Optional[str] = Field(None, env="ANTHROPIC_API_KEY")
    default_llm_provider: str = Field(default="openai", env="DEFAULT_LLM_PROVIDER")
    
    # å®‰å…¨é…ç½®
    secret_key: str = Field(..., env="SECRET_KEY")
    api_key: Optional[str] = Field(None, env="API_KEY")
    allowed_origins: List[str] = Field(default=["*"], env="ALLOWED_ORIGINS")
    
    # MCPé…ç½®
    mcp_sse_url: str = Field(default="http://localhost:8080", env="MCP_SSE_URL")
    mcp_api_key: Optional[str] = Field(None, env="MCP_API_KEY")
    
    # æ—¥å¿—é…ç½®
    log_level: str = Field(default="INFO", env="LOG_LEVEL")
    log_format: str = Field(default="json", env="LOG_FORMAT")
    log_file: Optional[str] = Field(None, env="LOG_FILE")
    
    # æ€§èƒ½é…ç½®
    max_connections: int = Field(default=100, env="MAX_CONNECTIONS")
    connection_timeout: int = Field(default=30, env="CONNECTION_TIMEOUT")
    request_timeout: int = Field(default=60, env="REQUEST_TIMEOUT")
    
    # ç¼“å­˜é…ç½®
    cache_ttl: int = Field(default=3600, env="CACHE_TTL")
    cache_max_size: int = Field(default=1000, env="CACHE_MAX_SIZE")
    
    @validator("allowed_origins", pre=True)
    def parse_allowed_origins(cls, v):
        if isinstance(v, str):
            return [origin.strip() for origin in v.split(",")]
        return v
    
    @validator("log_level")
    def validate_log_level(cls, v):
        valid_levels = ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]
        if v.upper() not in valid_levels:
            raise ValueError(f"æ—¥å¿—çº§åˆ«å¿…é¡»æ˜¯: {', '.join(valid_levels)}")
        return v.upper()
    
    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"
        case_sensitive = False

# å…¨å±€é…ç½®å®ä¾‹
settings = Settings()
```

#### 4. æ•°æ®åº“åˆå§‹åŒ–è„šæœ¬
```sql
-- init-scripts/01-init-database.sql
-- åˆ›å»ºæ•°æ®åº“å’Œç”¨æˆ·
CREATE DATABASE mirix;
CREATE USER mirix_user WITH PASSWORD 'your_postgres_password';
GRANT ALL PRIVILEGES ON DATABASE mirix TO mirix_user;

-- åˆ‡æ¢åˆ°mirixæ•°æ®åº“
\c mirix;

-- å¯ç”¨å¿…è¦çš„æ‰©å±•
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pgcrypto";
CREATE EXTENSION IF NOT EXISTS "vector";
CREATE EXTENSION IF NOT EXISTS "pg_trgm";

-- åˆ›å»ºå…¨æ–‡æœç´¢é…ç½®
CREATE TEXT SEARCH CONFIGURATION chinese (COPY = simple);

-- è®¾ç½®æ•°æ®åº“å‚æ•°
ALTER DATABASE mirix SET timezone TO 'UTC';
ALTER DATABASE mirix SET default_text_search_config TO 'chinese';

-- åˆ›å»ºåŸºç¡€è¡¨ç»“æ„
CREATE TABLE IF NOT EXISTS organizations (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(255) NOT NULL,
    description TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    is_deleted BOOLEAN DEFAULT FALSE
);

CREATE TABLE IF NOT EXISTS users (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(255) NOT NULL,
    email VARCHAR(255) UNIQUE,
    organization_id UUID REFERENCES organizations(id),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    is_deleted BOOLEAN DEFAULT FALSE
);

CREATE TABLE IF NOT EXISTS agents (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(255) NOT NULL,
    agent_type VARCHAR(50) NOT NULL,
    llm_config JSONB NOT NULL,
    memory_config JSONB,
    system_prompt TEXT,
    persona TEXT,
    user_id UUID REFERENCES users(id),
    organization_id UUID REFERENCES organizations(id),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    is_deleted BOOLEAN DEFAULT FALSE
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_users_organization_id ON users(organization_id);
CREATE INDEX idx_agents_user_id ON agents(user_id);
CREATE INDEX idx_agents_type ON agents(agent_type);
CREATE INDEX idx_agents_created_at ON agents(created_at);

-- åˆ›å»ºæ›´æ–°æ—¶é—´è§¦å‘å™¨
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ language 'plpgsql';

CREATE TRIGGER update_organizations_updated_at BEFORE UPDATE ON organizations FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
CREATE TRIGGER update_users_updated_at BEFORE UPDATE ON users FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
CREATE TRIGGER update_agents_updated_at BEFORE UPDATE ON agents FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
```

### ğŸ”§ å¼€å‘ç¯å¢ƒé…ç½®

#### 1. æœ¬åœ°å¼€å‘ç¯å¢ƒæ­å»º
```bash
#!/bin/bash
# scripts/setup-dev.sh - å¼€å‘ç¯å¢ƒæ­å»ºè„šæœ¬

set -e

echo "ğŸš€ å¼€å§‹è®¾ç½®MIRIXå¼€å‘ç¯å¢ƒ..."

# æ£€æŸ¥Pythonç‰ˆæœ¬
python_version=$(python3 --version 2>&1 | cut -d' ' -f2)
required_version="3.11"

if [[ $(echo "$python_version $required_version" | tr ' ' '\n' | sort -V | head -n1) != "$required_version" ]]; then
    echo "âŒ Pythonç‰ˆæœ¬éœ€è¦ >= $required_versionï¼Œå½“å‰ç‰ˆæœ¬: $python_version"
    exit 1
fi

echo "âœ… Pythonç‰ˆæœ¬æ£€æŸ¥é€šè¿‡: $python_version"

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
if [ ! -d "venv" ]; then
    echo "ğŸ“¦ åˆ›å»ºPythonè™šæ‹Ÿç¯å¢ƒ..."
    python3 -m venv venv
fi

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
echo "ğŸ”„ æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ..."
source venv/bin/activate

# å‡çº§pip
echo "â¬†ï¸ å‡çº§pip..."
pip install --upgrade pip

# å®‰è£…ä¾èµ–
echo "ğŸ“š å®‰è£…Pythonä¾èµ–..."
pip install -r requirements.txt
pip install -r requirements-dev.txt

# æ£€æŸ¥Docker
if ! command -v docker &> /dev/null; then
    echo "âŒ Dockeræœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…Docker"
    exit 1
fi

if ! command -v docker-compose &> /dev/null; then
    echo "âŒ Docker Composeæœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…Docker Compose"
    exit 1
fi

echo "âœ… Dockeræ£€æŸ¥é€šè¿‡"

# å¤åˆ¶ç¯å¢ƒå˜é‡æ–‡ä»¶
if [ ! -f ".env" ]; then
    echo "ğŸ“ åˆ›å»ºç¯å¢ƒå˜é‡æ–‡ä»¶..."
    cp .env.example .env
    echo "âš ï¸ è¯·ç¼–è¾‘ .env æ–‡ä»¶ï¼Œå¡«å…¥æ­£ç¡®çš„é…ç½®å€¼"
fi

# å¯åŠ¨æ•°æ®åº“æœåŠ¡
echo "ğŸ—„ï¸ å¯åŠ¨æ•°æ®åº“æœåŠ¡..."
docker-compose up -d postgres redis

# ç­‰å¾…æ•°æ®åº“å¯åŠ¨
echo "â³ ç­‰å¾…æ•°æ®åº“å¯åŠ¨..."
sleep 10

# è¿è¡Œæ•°æ®åº“è¿ç§»
echo "ğŸ”„ è¿è¡Œæ•°æ®åº“è¿ç§»..."
python -m alembic upgrade head

# åˆ›å»ºå¿…è¦ç›®å½•
echo "ğŸ“ åˆ›å»ºå¿…è¦ç›®å½•..."
mkdir -p logs uploads config

# å®‰è£…pre-commité’©å­
echo "ğŸ”§ å®‰è£…pre-commité’©å­..."
pre-commit install

echo "ğŸ‰ å¼€å‘ç¯å¢ƒè®¾ç½®å®Œæˆï¼"
echo ""
echo "ğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œï¼š"
echo "1. ç¼–è¾‘ .env æ–‡ä»¶ï¼Œå¡«å…¥APIå¯†é’¥ç­‰é…ç½®"
echo "2. è¿è¡Œ 'python -m uvicorn mirix.server.fastapi_server:app --reload' å¯åŠ¨å¼€å‘æœåŠ¡å™¨"
echo "3. è®¿é—® http://localhost:47283/docs æŸ¥çœ‹APIæ–‡æ¡£"
echo ""
echo "ğŸ› ï¸ å¸¸ç”¨å¼€å‘å‘½ä»¤ï¼š"
echo "- å¯åŠ¨å¼€å‘æœåŠ¡å™¨: make dev"
echo "- è¿è¡Œæµ‹è¯•: make test"
echo "- ä»£ç æ ¼å¼åŒ–: make format"
echo "- ç±»å‹æ£€æŸ¥: make typecheck"
```

#### 2. Makefileå¼€å‘å·¥å…·
```makefile
# Makefile - å¼€å‘å·¥å…·å‘½ä»¤
.PHONY: help dev test format typecheck lint clean build docker-build docker-run

# é»˜è®¤ç›®æ ‡
help:
	@echo "MIRIXå¼€å‘å·¥å…·å‘½ä»¤ï¼š"
	@echo "  dev          - å¯åŠ¨å¼€å‘æœåŠ¡å™¨"
	@echo "  test         - è¿è¡Œæµ‹è¯•"
	@echo "  format       - ä»£ç æ ¼å¼åŒ–"
	@echo "  typecheck    - ç±»å‹æ£€æŸ¥"
	@echo "  lint         - ä»£ç æ£€æŸ¥"
	@echo "  clean        - æ¸…ç†ä¸´æ—¶æ–‡ä»¶"
	@echo "  build        - æ„å»ºåº”ç”¨"
	@echo "  docker-build - æ„å»ºDockeré•œåƒ"
	@echo "  docker-run   - è¿è¡ŒDockerå®¹å™¨"

# å¯åŠ¨å¼€å‘æœåŠ¡å™¨
dev:
	@echo "ğŸš€ å¯åŠ¨å¼€å‘æœåŠ¡å™¨..."
	python -m uvicorn mirix.server.fastapi_server:app --reload --host 0.0.0.0 --port 47283

# è¿è¡Œæµ‹è¯•
test:
	@echo "ğŸ§ª è¿è¡Œæµ‹è¯•..."
	python -m pytest tests/ -v --cov=mirix --cov-report=html --cov-report=term

# ä»£ç æ ¼å¼åŒ–
format:
	@echo "ğŸ¨ ä»£ç æ ¼å¼åŒ–..."
	black mirix/ tests/
	isort mirix/ tests/

# ç±»å‹æ£€æŸ¥
typecheck:
	@echo "ğŸ” ç±»å‹æ£€æŸ¥..."
	mypy mirix/

# ä»£ç æ£€æŸ¥
lint:
	@echo "ğŸ” ä»£ç æ£€æŸ¥..."
	flake8 mirix/ tests/
	pylint mirix/

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
clean:
	@echo "ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶..."
	find . -type f -name "*.pyc" -delete
	find . -type d -name "__pycache__" -delete
	find . -type d -name "*.egg-info" -exec rm -rf {} +
	rm -rf build/ dist/ .coverage htmlcov/

# æ„å»ºåº”ç”¨
build:
	@echo "ğŸ—ï¸ æ„å»ºåº”ç”¨..."
	python -m build

# æ„å»ºDockeré•œåƒ
docker-build:
	@echo "ğŸ³ æ„å»ºDockeré•œåƒ..."
	docker build -t mirix:latest .

# è¿è¡ŒDockerå®¹å™¨
docker-run:
	@echo "ğŸ³ è¿è¡ŒDockerå®¹å™¨..."
	docker-compose up -d

# åœæ­¢Dockerå®¹å™¨
docker-stop:
	@echo "ğŸ›‘ åœæ­¢Dockerå®¹å™¨..."
	docker-compose down

# æ•°æ®åº“è¿ç§»
migrate:
	@echo "ğŸ”„ è¿è¡Œæ•°æ®åº“è¿ç§»..."
	python -m alembic upgrade head

# åˆ›å»ºæ–°çš„æ•°æ®åº“è¿ç§»
migration:
	@echo "ğŸ“ åˆ›å»ºæ•°æ®åº“è¿ç§»..."
	python -m alembic revision --autogenerate -m "$(MSG)"

# å®‰è£…ä¾èµ–
install:
	@echo "ğŸ“¦ å®‰è£…ä¾èµ–..."
	pip install -r requirements.txt
	pip install -r requirements-dev.txt

# æ›´æ–°ä¾èµ–
update:
	@echo "â¬†ï¸ æ›´æ–°ä¾èµ–..."
	pip-compile requirements.in
	pip-compile requirements-dev.in
	pip install -r requirements.txt
	pip install -r requirements-dev.txt
```

---

## ğŸ“Š ç›‘æ§å’Œè¿ç»´

### æ€§èƒ½ç›‘æ§é…ç½®
```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "rules/*.yml"

scrape_configs:
  - job_name: 'mirix-backend'
    static_configs:
      - targets: ['mirix-backend:47283']
    metrics_path: '/metrics'
    scrape_interval: 30s

  - job_name: 'mcp-sse-service'
    static_configs:
      - targets: ['mcp-sse-service:8080']
    metrics_path: '/metrics'
    scrape_interval: 30s

  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres:5432']

  - job_name: 'redis'
    static_configs:
      - targets: ['redis:6379']
```

### æ—¥å¿—ç®¡ç†é…ç½®
```python
# config/logging.py
import logging
import logging.config
from typing import Dict, Any

LOGGING_CONFIG: Dict[str, Any] = {
    "version": 1,
    "disable_existing_loggers": False,
    "formatters": {
        "default": {
            "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        },
        "json": {
            "()": "pythonjsonlogger.jsonlogger.JsonFormatter",
            "format": "%(asctime)s %(name)s %(levelname)s %(message)s",
        },
    },
    "handlers": {
        "console": {
            "class": "logging.StreamHandler",
            "level": "INFO",
            "formatter": "default",
            "stream": "ext://sys.stdout",
        },
        "file": {
            "class": "logging.handlers.RotatingFileHandler",
            "level": "DEBUG",
            "formatter": "json",
            "filename": "logs/mirix.log",
            "maxBytes": 10485760,  # 10MB
            "backupCount": 5,
        },
        "error_file": {
            "class": "logging.handlers.RotatingFileHandler",
            "level": "ERROR",
            "formatter": "json",
            "filename": "logs/mirix_error.log",
            "maxBytes": 10485760,  # 10MB
            "backupCount": 5,
        },
    },
    "loggers": {
        "mirix": {
            "level": "DEBUG",
            "handlers": ["console", "file", "error_file"],
            "propagate": False,
        },
        "uvicorn": {
            "level": "INFO",
            "handlers": ["console", "file"],
            "propagate": False,
        },
        "sqlalchemy": {
            "level": "WARNING",
            "handlers": ["console", "file"],
            "propagate": False,
        },
    },
    "root": {
        "level": "INFO",
        "handlers": ["console"],
    },
}

def setup_logging():
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    logging.config.dictConfig(LOGGING_CONFIG)
```

### å®‰å…¨é…ç½®
```python
# config/security.py
from typing import List, Optional
import secrets
import hashlib
import hmac
from datetime import datetime, timedelta

class SecurityConfig:
    """å®‰å…¨é…ç½®ç®¡ç†"""
    
    def __init__(self):
        self.api_key_length = 32
        self.session_timeout = 3600  # 1å°æ—¶
        self.max_login_attempts = 5
        self.lockout_duration = 900  # 15åˆ†é’Ÿ
        
        # å¯†ç ç­–ç•¥
        self.min_password_length = 8
        self.require_uppercase = True
        self.require_lowercase = True
        self.require_numbers = True
        self.require_special_chars = True
    
    def generate_api_key(self) -> str:
        """ç”ŸæˆAPIå¯†é’¥"""
        return secrets.token_urlsafe(self.api_key_length)
    
    def hash_password(self, password: str, salt: Optional[str] = None) -> tuple[str, str]:
        """å¯†ç å“ˆå¸Œ"""
        if salt is None:
            salt = secrets.token_hex(16)
        
        password_hash = hashlib.pbkdf2_hmac(
            'sha256',
            password.encode('utf-8'),
            salt.encode('utf-8'),
            100000  # è¿­ä»£æ¬¡æ•°
        )
        
        return password_hash.hex(), salt
    
    def verify_password(self, password: str, password_hash: str, salt: str) -> bool:
        """éªŒè¯å¯†ç """
        computed_hash, _ = self.hash_password(password, salt)
        return hmac.compare_digest(computed_hash, password_hash)
    
    def validate_password_strength(self, password: str) -> List[str]:
        """éªŒè¯å¯†ç å¼ºåº¦"""
        errors = []
        
        if len(password) < self.min_password_length:
            errors.append(f"å¯†ç é•¿åº¦è‡³å°‘{self.min_password_length}ä½")
        
        if self.require_uppercase and not any(c.isupper() for c in password):
            errors.append("å¯†ç å¿…é¡»åŒ…å«å¤§å†™å­—æ¯")
        
        if self.require_lowercase and not any(c.islower() for c in password):
            errors.append("å¯†ç å¿…é¡»åŒ…å«å°å†™å­—æ¯")
        
        if self.require_numbers and not any(c.isdigit() for c in password):
            errors.append("å¯†ç å¿…é¡»åŒ…å«æ•°å­—")
        
        if self.require_special_chars and not any(c in "!@#$%^&*()_+-=[]{}|;:,.<>?" for c in password):
            errors.append("å¯†ç å¿…é¡»åŒ…å«ç‰¹æ®Šå­—ç¬¦")
        
        return errors

# å…¨å±€å®‰å…¨é…ç½®
security_config = SecurityConfig()
```

---

## ğŸš¨ æ•…éšœæ’æŸ¥å’Œè§£å†³æ–¹æ¡ˆ

### å¸¸è§é—®é¢˜è¯Šæ–­
```python
# utils/diagnostics.py
import asyncio
import psutil
import logging
from typing import Dict, Any, List
from datetime import datetime

class SystemDiagnostics:
    """ç³»ç»Ÿè¯Šæ–­å·¥å…·"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    async def run_full_diagnostics(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´ç³»ç»Ÿè¯Šæ–­"""
        diagnostics = {
            "timestamp": datetime.utcnow().isoformat(),
            "system": await self._check_system_resources(),
            "database": await self._check_database_connection(),
            "redis": await self._check_redis_connection(),
            "llm": await self._check_llm_services(),
            "memory": await self._check_memory_system(),
            "agents": await self._check_agent_system(),
        }
        
        return diagnostics
    
    async def _check_system_resources(self) -> Dict[str, Any]:
        """æ£€æŸ¥ç³»ç»Ÿèµ„æº"""
        return {
            "cpu_percent": psutil.cpu_percent(interval=1),
            "memory_percent": psutil.virtual_memory().percent,
            "disk_percent": psutil.disk_usage('/').percent,
            "load_average": psutil.getloadavg() if hasattr(psutil, 'getloadavg') else None,
        }
    
    async def _check_database_connection(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ•°æ®åº“è¿æ¥"""
        try:
            # è¿™é‡Œåº”è¯¥å®é™…æµ‹è¯•æ•°æ®åº“è¿æ¥
            return {
                "status": "healthy",
                "response_time_ms": 50,
                "active_connections": 10,
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
            }
    
    async def _check_redis_connection(self) -> Dict[str, Any]:
        """æ£€æŸ¥Redisè¿æ¥"""
        try:
            # è¿™é‡Œåº”è¯¥å®é™…æµ‹è¯•Redisè¿æ¥
            return {
                "status": "healthy",
                "response_time_ms": 5,
                "memory_usage": "50MB",
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
            }
    
    async def _check_llm_services(self) -> Dict[str, Any]:
        """æ£€æŸ¥LLMæœåŠ¡"""
        try:
            # è¿™é‡Œåº”è¯¥å®é™…æµ‹è¯•LLMæœåŠ¡
            return {
                "openai": {"status": "healthy", "response_time_ms": 1500},
                "anthropic": {"status": "healthy", "response_time_ms": 1200},
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
            }

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§"""
    
    def __init__(self):
        self.metrics = {}
        self.alerts = []
    
    async def collect_metrics(self):
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        self.metrics.update({
            "timestamp": datetime.utcnow().isoformat(),
            "response_times": await self._collect_response_times(),
            "throughput": await self._collect_throughput(),
            "error_rates": await self._collect_error_rates(),
            "resource_usage": await self._collect_resource_usage(),
        })
    
    async def check_alerts(self):
        """æ£€æŸ¥å‘Šè­¦æ¡ä»¶"""
        # å“åº”æ—¶é—´å‘Šè­¦
        avg_response_time = self.metrics.get("response_times", {}).get("average", 0)
        if avg_response_time > 5000:  # 5ç§’
            self.alerts.append({
                "type": "high_response_time",
                "message": f"å¹³å‡å“åº”æ—¶é—´è¿‡é«˜: {avg_response_time}ms",
                "severity": "warning",
                "timestamp": datetime.utcnow().isoformat(),
            })
        
        # é”™è¯¯ç‡å‘Šè­¦
        error_rate = self.metrics.get("error_rates", {}).get("total", 0)
        if error_rate > 0.05:  # 5%
            self.alerts.append({
                "type": "high_error_rate",
                "message": f"é”™è¯¯ç‡è¿‡é«˜: {error_rate*100:.2f}%",
                "severity": "critical",
                "timestamp": datetime.utcnow().isoformat(),
            })
```

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### æ•°æ®åº“ä¼˜åŒ–
```sql
-- æ€§èƒ½ä¼˜åŒ–SQLè„šæœ¬
-- åˆ›å»ºå¤åˆç´¢å¼•
CREATE INDEX CONCURRENTLY idx_messages_agent_created 
ON messages(agent_id, created_at DESC) 
WHERE is_deleted = FALSE;

-- åˆ›å»ºéƒ¨åˆ†ç´¢å¼•
CREATE INDEX CONCURRENTLY idx_active_agents 
ON agents(user_id, agent_type) 
WHERE is_deleted = FALSE;

-- åˆ›å»ºå‘é‡ç´¢å¼•
CREATE INDEX CONCURRENTLY idx_memory_embedding 
ON memory_items USING ivfflat (embedding vector_cosine_ops) 
WITH (lists = 100);

-- åˆ›å»ºå…¨æ–‡æœç´¢ç´¢å¼•
CREATE INDEX CONCURRENTLY idx_memory_content_fts 
ON memory_items USING gin(to_tsvector('chinese', content));

-- åˆ†åŒºè¡¨è®¾ç½®ï¼ˆæŒ‰æ—¶é—´åˆ†åŒºï¼‰
CREATE TABLE messages_y2024m01 PARTITION OF messages 
FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');

-- å®šæœŸç»´æŠ¤è„šæœ¬
-- æ›´æ–°è¡¨ç»Ÿè®¡ä¿¡æ¯
ANALYZE;

-- é‡å»ºç´¢å¼•
REINDEX INDEX CONCURRENTLY idx_messages_agent_created;

-- æ¸…ç†æ— ç”¨æ•°æ®
DELETE FROM messages 
WHERE is_deleted = TRUE 
AND updated_at < NOW() - INTERVAL '30 days';
```

### ç¼“å­˜ç­–ç•¥
```python
# utils/cache.py
import redis
import json
import pickle
from typing import Any, Optional, Union
from datetime import timedelta

class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        self.default_ttl = 3600  # 1å°æ—¶
    
    async def get(self, key: str, default: Any = None) -> Any:
        """è·å–ç¼“å­˜"""
        try:
            value = await self.redis.get(key)
            if value is None:
                return default
            
            # å°è¯•JSONè§£æï¼Œå¤±è´¥åˆ™ä½¿ç”¨pickle
            try:
                return json.loads(value)
            except json.JSONDecodeError:
                return pickle.loads(value)
        except Exception:
            return default
    
    async def set(
        self, 
        key: str, 
        value: Any, 
        ttl: Optional[Union[int, timedelta]] = None
    ) -> bool:
        """è®¾ç½®ç¼“å­˜"""
        try:
            # å°è¯•JSONåºåˆ—åŒ–ï¼Œå¤±è´¥åˆ™ä½¿ç”¨pickle
            try:
                serialized_value = json.dumps(value)
            except (TypeError, ValueError):
                serialized_value = pickle.dumps(value)
            
            ttl_seconds = ttl or self.default_ttl
            if isinstance(ttl_seconds, timedelta):
                ttl_seconds = int(ttl_seconds.total_seconds())
            
            return await self.redis.setex(key, ttl_seconds, serialized_value)
        except Exception:
            return False
    
    async def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜"""
        try:
            return bool(await self.redis.delete(key))
        except Exception:
            return False
    
    async def exists(self, key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨"""
        try:
            return bool(await self.redis.exists(key))
        except Exception:
            return False
    
    def cache_key(self, prefix: str, *args) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_parts = [prefix] + [str(arg) for arg in args]
        return ":".join(key_parts)

# ç¼“å­˜è£…é¥°å™¨
def cached(ttl: int = 3600, key_prefix: str = ""):
    """ç¼“å­˜è£…é¥°å™¨"""
    def decorator(func):
        async def wrapper(*args, **kwargs):
            cache_manager = get_cache_manager()  # è·å–ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
            
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = cache_manager.cache_key(
                key_prefix or func.__name__,
                *args,
                **sorted(kwargs.items())
            )
            
            # å°è¯•ä»ç¼“å­˜è·å–
            cached_result = await cache_manager.get(cache_key)
            if cached_result is not None:
                return cached_result
            
            # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ
            result = await func(*args, **kwargs)
            await cache_manager.set(cache_key, result, ttl)
            
            return result
        
        return wrapper
    return decorator
```

---

## ğŸ”„ æŒç»­é›†æˆå’Œéƒ¨ç½²

### CI/CDé…ç½®
```yaml
# .github/workflows/ci-cd.yml
name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: pgvector/pgvector:pg16
        env:
          POSTGRES_PASSWORD: test_password
          POSTGRES_DB: mirix_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
    
    - name: Run tests
      env:
        DATABASE_URL: postgresql://postgres:test_password@localhost:5432/mirix_test
        REDIS_URL: redis://localhost:6379/0
      run: |
        python -m pytest tests/ -v --cov=mirix --cov-report=xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
    
    - name: Run type checking
      run: mypy mirix/
    
    - name: Run linting
      run: |
        flake8 mirix/ tests/
        pylint mirix/

  build:
    needs: test
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2
    
    - name: Login to Docker Hub
      uses: docker/login-action@v2
      with:
        username: ${{ secrets.DOCKER_USERNAME }}
        password: ${{ secrets.DOCKER_PASSWORD }}
    
    - name: Build and push Docker image
      uses: docker/build-push-action@v4
      with:
        context: .
        push: true
        tags: |
          mirix/mirix:latest
          mirix/mirix:${{ github.sha }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to production
      run: |
        echo "éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ"
        # è¿™é‡Œæ·»åŠ å®é™…çš„éƒ¨ç½²è„šæœ¬
```

---

## ğŸ“š æ€»ç»“

MIRIXç³»ç»Ÿæ¶æ„è®¾è®¡æ–‡æ¡£æä¾›äº†å®Œæ•´çš„ä¸‰å±‚çŸ¥è¯†ä½“ç³»ï¼š

1. **ç¬¬ä¸€å±‚ - æ•´ä½“æ¶æ„å¤§çº²**ï¼šç³»ç»Ÿå…¨æ™¯å›¾ã€æ ¸å¿ƒè®¾è®¡åŸåˆ™ã€æŠ€æœ¯æ ˆé€‰å‹ã€æ•°æ®æµæ¶æ„
2. **ç¬¬äºŒå±‚ - æŠ€æœ¯è®¾è®¡æ”¯æŒ**ï¼šåˆ†å±‚æ¶æ„è®¾è®¡ã€é›†æˆæ–¹æ¡ˆè®¾è®¡ã€LLMé›†æˆã€MCPåè®®é›†æˆ
3. **ç¬¬ä¸‰å±‚ - å®æ–½æŒ‡å¯¼**ï¼šéƒ¨ç½²æ¶æ„ã€å¼€å‘ç¯å¢ƒé…ç½®ã€ç›‘æ§è¿ç»´ã€æ•…éšœæ’æŸ¥ã€æ€§èƒ½ä¼˜åŒ–

è¯¥æ¶æ„è®¾è®¡ç¡®ä¿äº†MIRIXç³»ç»Ÿçš„ï¼š
- **å¯æ‰©å±•æ€§**ï¼šæ”¯æŒæ°´å¹³å’Œå‚ç›´æ‰©å±•
- **å¯ç»´æŠ¤æ€§**ï¼šæ¨¡å—åŒ–è®¾è®¡ï¼Œä¾¿äºç»´æŠ¤å’Œå‡çº§
- **é«˜æ€§èƒ½**ï¼šå¼‚æ­¥å¤„ç†ã€ç¼“å­˜ç­–ç•¥ã€æ•°æ®åº“ä¼˜åŒ–
- **å®‰å…¨æ€§**ï¼šå¤šå±‚å®‰å…¨é˜²æŠ¤ã€æ•°æ®åŠ å¯†ã€è®¿é—®æ§åˆ¶
- **å¯è§‚æµ‹æ€§**ï¼šå®Œæ•´çš„ç›‘æ§ã€æ—¥å¿—ã€å‘Šè­¦ä½“ç³»

é€šè¿‡è¿™ä¸ªæ¶æ„è®¾è®¡ï¼Œå¼€å‘å›¢é˜Ÿå¯ä»¥ï¼š
- ç†è§£ç³»ç»Ÿçš„æ•´ä½“è®¾è®¡æ€è·¯å’ŒæŠ€æœ¯é€‰å‹
- è·å¾—å…·ä½“çš„æŠ€æœ¯å®æ–½æŒ‡å¯¼
- æŒæ¡ç³»ç»Ÿçš„éƒ¨ç½²ã€ç›‘æ§å’Œè¿ç»´æ–¹æ³•
- è§£å†³å¸¸è§çš„æŠ€æœ¯é—®é¢˜å’Œæ€§èƒ½ä¼˜åŒ–